This project explores hyperparameter tuning of RoBERTa and BERTweet models on the cardiffnlp/tweet_eval irony detection dataset. It features:
  - Tools Used: Colab, with manual fine-tuning and Optuna for optimization.
  - Models: RoBERTa and BERTweet fine-tuned for irony detection.
  - Notes: Adjustments and reruns led to minor “error” messages, which are harmless and stem from cleaning up code post-completion.

A robust and well-performing implementation of NLP techniques for irony detection.
